{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "708b8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set professional plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd21fd",
   "metadata": {},
   "source": [
    "### LOAD ALL THREE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4fb1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained models...\n",
      "   ‚ö†Ô∏è  Model not found: ..\\xgboost_model\\xgb_model_v1.pkl\n",
      "   ‚ö†Ô∏è  Model not found: ..\\catboost_model\\cat_model_v1.pkl\n",
      "\n",
      "‚úÖ Model Loading Status:\n",
      "   LightGBM    : ‚úì Loaded\n",
      "   XGBoost     : ‚úó Missing\n",
      "   CatBoost    : ‚úó Missing\n",
      "\n",
      "‚ö†Ô∏è  Warning: Not all models loaded. Train missing models first!\n",
      "   Ensemble will use only available models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = f'../../data'\n",
    "MODEL_PATH = f'.'\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ö†Ô∏è  Model not found: {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Load models\n",
    "print(\"Loading trained models...\")\n",
    "lgb_model = load_model(fr'..\\lightgbm_model\\lgb_model_v1.pkl')\n",
    "xgb_model = load_model(fr'..\\xgboost_model\\xgb_model_v1.pkl')\n",
    "cat_model = load_model(fr'..\\catboost_model\\cat_model_v1.pkl')\n",
    "\n",
    "# Check which models loaded successfully\n",
    "models_loaded = {\n",
    "    'LightGBM': lgb_model is not None,\n",
    "    'XGBoost': xgb_model is not None,\n",
    "    'CatBoost': cat_model is not None\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Model Loading Status:\")\n",
    "for name, loaded in models_loaded.items():\n",
    "    status = \"‚úì Loaded\" if loaded else \"‚úó Missing\"\n",
    "    print(f\"   {name:12s}: {status}\")\n",
    "\n",
    "if not all(models_loaded.values()):\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: Not all models loaded. Train missing models first!\")\n",
    "    print(\"   Ensemble will use only available models.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa87be7",
   "metadata": {},
   "source": [
    "### LOAD VALIDATION & TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7587775",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'koi_depth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m     \u001b[38;5;66;03m# 1. TRANSIT STRENGTH (interaction: depth √ó SNR)\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# High values = strong signal with high confidence\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_new[\u001b[33m'\u001b[39m\u001b[33mtransit_strength\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mkoi_depth\u001b[49m * koi_snr\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# 2. DETECTION CONFIDENCE (just renaming for clarity)\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m df_new[\u001b[33m'\u001b[39m\u001b[33mdetection_confidence\u001b[39m\u001b[33m'\u001b[39m] = koi_snr\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'koi_depth' is not defined"
     ]
    }
   ],
   "source": [
    "# Load preprocessed datasets\n",
    "X_train = pd.read_csv(f'{DATA_PATH}/step6_X_train.csv')\n",
    "y_train = pd.read_csv(f'{DATA_PATH}/step6_y_train.csv').squeeze()\n",
    "\n",
    "X_validate = pd.read_csv(f'{DATA_PATH}/step6_X_val.csv')\n",
    "y_validate = pd.read_csv(f'{DATA_PATH}/step6_y_val.csv').squeeze()\n",
    "\n",
    "X_test = pd.read_csv(f'{DATA_PATH}/step6_X_test.csv')\n",
    "y_test = pd.read_csv(f'{DATA_PATH}/step6_y_test.csv').squeeze()\n",
    "\n",
    "# Apply feature engineering (MUST match training!)\n",
    "def engineer_transit_features(X):\n",
    "    \"\"\"Apply transit method feature engineering - MUST MATCH TRAINING!\"\"\"\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    # 1. Depth-Duration Interaction\n",
    "    if 'koi_depth' in X.columns and 'koi_duration' in X.columns:\n",
    "        X_enhanced['transit_depth_duration_ratio'] = (\n",
    "            X['koi_depth'] / (X['koi_duration'] + 1e-6)\n",
    "        )\n",
    "    \n",
    "    # 2. Signal Quality Score\n",
    "    if 'koi_model_snr' in X.columns:\n",
    "        X_enhanced['snr_log'] = np.log1p(X['koi_model_snr'])\n",
    "        X_enhanced['snr_squared'] = X['koi_model_snr'] ** 2\n",
    "    \n",
    "    # 3. Transit Detectability Index\n",
    "    if all(f in X.columns for f in ['koi_duration', 'koi_depth', 'koi_model_snr']):\n",
    "        X_enhanced['transit_detectability'] = (\n",
    "            X['koi_depth'] * X['koi_model_snr'] / (X['koi_duration'] + 1)\n",
    "        )\n",
    "    \n",
    "    # 4. Normalized transit depth\n",
    "    if 'koi_depth' in X.columns:\n",
    "        depth_median = X['koi_depth'].median()\n",
    "        X_enhanced['depth_normalized'] = X['koi_depth'] / depth_median\n",
    "    \n",
    "    return X_enhanced\n",
    "\n",
    "print(\"Applying feature engineering...\")\n",
    "X_train_enhanced = engineer_transit_features(X_train)\n",
    "X_validate_enhanced = engineer_transit_features(X_validate)\n",
    "X_test_enhanced = engineer_transit_features(X_test)\n",
    "\n",
    "print(f\"   Training:   {X_train_enhanced.shape}\")\n",
    "print(f\"   Validation: {X_validate_enhanced.shape}\")\n",
    "print(f\"   Test:       {X_test_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4544d8",
   "metadata": {},
   "source": [
    "### INDIVIDUAL MODEL PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X, model_type='lightgbm'):\n",
    "    \"\"\"Get predictions from a model with proper handling for each type.\"\"\"\n",
    "    if model is None:\n",
    "        return None, None\n",
    "    \n",
    "    if model_type == 'lightgbm':\n",
    "        # LightGBM: predict returns probabilities directly\n",
    "        proba = model.predict(X, num_iteration=model.best_iteration)\n",
    "        pred = proba.argmax(axis=1)\n",
    "    elif model_type == 'xgboost':\n",
    "        # XGBoost: predict_proba for probabilities\n",
    "        proba = model.predict_proba(X)\n",
    "        pred = proba.argmax(axis=1)\n",
    "    elif model_type == 'catboost':\n",
    "        # CatBoost: predict with type='Probability'\n",
    "        proba = model.predict(X, prediction_type='Probability')\n",
    "        pred = proba.argmax(axis=1)\n",
    "    \n",
    "    return pred, proba\n",
    "\n",
    "# Get validation predictions\n",
    "print(\"Generating predictions on VALIDATION set...\\n\")\n",
    "\n",
    "lgb_val_pred, lgb_val_proba = get_predictions(lgb_model, X_validate_enhanced, 'lightgbm')\n",
    "xgb_val_pred, xgb_val_proba = get_predictions(xgb_model, X_validate_enhanced, 'xgboost')\n",
    "cat_val_pred, cat_val_proba = get_predictions(cat_model, X_validate_enhanced, 'catboost')\n",
    "\n",
    "# Get test predictions\n",
    "print(\"Generating predictions on TEST set...\\n\")\n",
    "\n",
    "lgb_test_pred, lgb_test_proba = get_predictions(lgb_model, X_test_enhanced, 'lightgbm')\n",
    "xgb_test_pred, xgb_test_proba = get_predictions(xgb_model, X_test_enhanced, 'xgboost')\n",
    "cat_test_pred, cat_test_proba = get_predictions(cat_model, X_test_enhanced, 'catboost')\n",
    "\n",
    "# Evaluate individual models on validation\n",
    "print(\"üìä Individual Model Performance (Validation Set):\\n\")\n",
    "\n",
    "class_names = ['False Positive', 'Candidate', 'Confirmed']\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate a single model.\"\"\"\n",
    "    if y_pred is None:\n",
    "        return None\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"   Accuracy:    {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    print(f\"   Precision:   {precision:.4f}\")\n",
    "    print(f\"   Recall:      {recall:.4f}\")\n",
    "    print(f\"   F1 (Macro):  {f1_macro:.4f}\")\n",
    "    print(f\"   F1 (Weight): {f1_weighted:.4f}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "lgb_metrics = evaluate_model(y_validate, lgb_val_pred, \"LightGBM\")\n",
    "xgb_metrics = evaluate_model(y_validate, xgb_val_pred, \"XGBoost\")\n",
    "cat_metrics = evaluate_model(y_validate, cat_val_pred, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d71f5",
   "metadata": {},
   "source": [
    "### ENSEMBLE METHOD 1 - SIMPLE AVERAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strategy: Average probability predictions from all models\\n\")\n",
    "\n",
    "def simple_average_ensemble(proba_list):\n",
    "    \"\"\"Average predictions from multiple models.\"\"\"\n",
    "    valid_probas = [p for p in proba_list if p is not None]\n",
    "    if not valid_probas:\n",
    "        return None, None\n",
    "    \n",
    "    avg_proba = np.mean(valid_probas, axis=0)\n",
    "    predictions = avg_proba.argmax(axis=1)\n",
    "    return predictions, avg_proba\n",
    "\n",
    "# Validation ensemble\n",
    "ensemble_avg_val_pred, ensemble_avg_val_proba = simple_average_ensemble([\n",
    "    lgb_val_proba, xgb_val_proba, cat_val_proba\n",
    "])\n",
    "\n",
    "# Test ensemble\n",
    "ensemble_avg_test_pred, ensemble_avg_test_proba = simple_average_ensemble([\n",
    "    lgb_test_proba, xgb_test_proba, cat_test_proba\n",
    "])\n",
    "\n",
    "print(\"üìä Simple Average Ensemble Performance:\\n\")\n",
    "print(\"VALIDATION SET:\")\n",
    "avg_val_metrics = evaluate_model(y_validate, ensemble_avg_val_pred, \"Ensemble (Avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdcab7",
   "metadata": {},
   "source": [
    "### ENSEMBLE METHOD 2 - WEIGHTED AVERAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strategy: Weight each model by its validation F1-score\\n\")\n",
    "\n",
    "# Calculate optimal weights based on validation performance\n",
    "weights_dict = {}\n",
    "total_f1 = 0\n",
    "\n",
    "if lgb_metrics:\n",
    "    weights_dict['LightGBM'] = lgb_metrics['f1_weighted']\n",
    "    total_f1 += lgb_metrics['f1_weighted']\n",
    "if xgb_metrics:\n",
    "    weights_dict['XGBoost'] = xgb_metrics['f1_weighted']\n",
    "    total_f1 += xgb_metrics['f1_weighted']\n",
    "if cat_metrics:\n",
    "    weights_dict['CatBoost'] = cat_metrics['f1_weighted']\n",
    "    total_f1 += cat_metrics['f1_weighted']\n",
    "\n",
    "# Normalize weights\n",
    "normalized_weights = {k: v/total_f1 for k, v in weights_dict.items()}\n",
    "\n",
    "print(\"Computed Model Weights (based on validation F1-score):\")\n",
    "for model, weight in normalized_weights.items():\n",
    "    print(f\"   {model:12s}: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "\n",
    "def weighted_average_ensemble(proba_list, weights):\n",
    "    \"\"\"Weighted average of predictions.\"\"\"\n",
    "    weighted_sum = np.zeros_like(proba_list[0])\n",
    "    total_weight = 0\n",
    "    \n",
    "    for i, (proba, weight) in enumerate(zip(proba_list, weights)):\n",
    "        if proba is not None:\n",
    "            weighted_sum += proba * weight\n",
    "            total_weight += weight\n",
    "    \n",
    "    avg_proba = weighted_sum / total_weight\n",
    "    predictions = avg_proba.argmax(axis=1)\n",
    "    return predictions, avg_proba\n",
    "\n",
    "# Extract weights in order\n",
    "weight_values = [\n",
    "    normalized_weights.get('LightGBM', 0),\n",
    "    normalized_weights.get('XGBoost', 0),\n",
    "    normalized_weights.get('CatBoost', 0)\n",
    "]\n",
    "\n",
    "# Validation ensemble\n",
    "ensemble_weighted_val_pred, ensemble_weighted_val_proba = weighted_average_ensemble(\n",
    "    [lgb_val_proba, xgb_val_proba, cat_val_proba],\n",
    "    weight_values\n",
    ")\n",
    "\n",
    "# Test ensemble\n",
    "ensemble_weighted_test_pred, ensemble_weighted_test_proba = weighted_average_ensemble(\n",
    "    [lgb_test_proba, xgb_test_proba, cat_test_proba],\n",
    "    weight_values\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Weighted Average Ensemble Performance:\\n\")\n",
    "print(\"VALIDATION SET:\")\n",
    "weighted_val_metrics = evaluate_model(y_validate, ensemble_weighted_val_pred, \"Ensemble (Weighted)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9066d3",
   "metadata": {},
   "source": [
    "### ENSEMBLE METHOD 3 - MAJORITY VOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec99b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strategy: Each model votes, majority wins (ties broken by confidence)\\n\")\n",
    "\n",
    "def majority_voting_ensemble(pred_list, proba_list):\n",
    "    \"\"\"Majority voting with tie-breaking.\"\"\"\n",
    "    valid_preds = [p for p in pred_list if p is not None]\n",
    "    valid_probas = [p for p in proba_list if p is not None]\n",
    "    \n",
    "    if not valid_preds:\n",
    "        return None\n",
    "    \n",
    "    # Stack predictions\n",
    "    votes = np.column_stack(valid_preds)\n",
    "    \n",
    "    # Majority vote\n",
    "    from scipy import stats\n",
    "    majority_pred, _ = stats.mode(votes, axis=1, keepdims=False)\n",
    "    \n",
    "    return majority_pred\n",
    "\n",
    "# Validation ensemble\n",
    "ensemble_vote_val_pred = majority_voting_ensemble(\n",
    "    [lgb_val_pred, xgb_val_pred, cat_val_pred],\n",
    "    [lgb_val_proba, xgb_val_proba, cat_val_proba]\n",
    ")\n",
    "\n",
    "# Test ensemble\n",
    "ensemble_vote_test_pred = majority_voting_ensemble(\n",
    "    [lgb_test_pred, xgb_test_pred, cat_test_pred],\n",
    "    [lgb_test_proba, xgb_test_proba, cat_test_proba]\n",
    ")\n",
    "\n",
    "print(\"üìä Majority Voting Ensemble Performance:\\n\")\n",
    "print(\"VALIDATION SET:\")\n",
    "vote_val_metrics = evaluate_model(y_validate, ensemble_vote_val_pred, \"Ensemble (Vote)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549fbc8",
   "metadata": {},
   "source": [
    "### ENSEMBLE METHOD 4 - STACKING META-LEARNER (ADVANCED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strategy: Train a meta-model to learn optimal combination\\n\")\n",
    "\n",
    "# Create meta-features (predictions from base models)\n",
    "meta_train_features = np.column_stack([\n",
    "    lgb_val_proba if lgb_val_proba is not None else np.zeros((len(y_validate), 3)),\n",
    "    xgb_val_proba if xgb_val_proba is not None else np.zeros((len(y_validate), 3)),\n",
    "    cat_val_proba if cat_val_proba is not None else np.zeros((len(y_validate), 3))\n",
    "])\n",
    "\n",
    "meta_test_features = np.column_stack([\n",
    "    lgb_test_proba if lgb_test_proba is not None else np.zeros((len(y_test), 3)),\n",
    "    xgb_test_proba if xgb_test_proba is not None else np.zeros((len(y_test), 3)),\n",
    "    cat_test_proba if cat_test_proba is not None else np.zeros((len(y_test), 3))\n",
    "])\n",
    "\n",
    "print(f\"Meta-features shape: {meta_train_features.shape}\")\n",
    "print(\"Training Logistic Regression meta-learner...\\n\")\n",
    "\n",
    "# Train meta-model (Logistic Regression)\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    random_state=42,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "meta_model.fit(meta_train_features, y_validate)\n",
    "\n",
    "# Predictions\n",
    "ensemble_stack_val_pred = meta_model.predict(meta_train_features)\n",
    "ensemble_stack_test_pred = meta_model.predict(meta_test_features)\n",
    "ensemble_stack_test_proba = meta_model.predict_proba(meta_test_features)\n",
    "\n",
    "print(\"üìä Stacking Meta-Learner Performance:\\n\")\n",
    "print(\"VALIDATION SET:\")\n",
    "stack_val_metrics = evaluate_model(y_validate, ensemble_stack_val_pred, \"Ensemble (Stack)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ecab47",
   "metadata": {},
   "source": [
    "### COMPARE ALL METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ed793",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'LightGBM (Solo)',\n",
    "        'XGBoost (Solo)',\n",
    "        'CatBoost (Solo)',\n",
    "        'Simple Average',\n",
    "        'Weighted Average',\n",
    "        'Majority Voting',\n",
    "        'Stacking Meta'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        lgb_metrics['accuracy'] if lgb_metrics else 0,\n",
    "        xgb_metrics['accuracy'] if xgb_metrics else 0,\n",
    "        cat_metrics['accuracy'] if cat_metrics else 0,\n",
    "        avg_val_metrics['accuracy'],\n",
    "        weighted_val_metrics['accuracy'],\n",
    "        vote_val_metrics['accuracy'],\n",
    "        stack_val_metrics['accuracy']\n",
    "    ],\n",
    "    'F1 (Macro)': [\n",
    "        lgb_metrics['f1_macro'] if lgb_metrics else 0,\n",
    "        xgb_metrics['f1_macro'] if xgb_metrics else 0,\n",
    "        cat_metrics['f1_macro'] if cat_metrics else 0,\n",
    "        avg_val_metrics['f1_macro'],\n",
    "        weighted_val_metrics['f1_macro'],\n",
    "        vote_val_metrics['f1_macro'],\n",
    "        stack_val_metrics['f1_macro']\n",
    "    ],\n",
    "    'F1 (Weighted)': [\n",
    "        lgb_metrics['f1_weighted'] if lgb_metrics else 0,\n",
    "        xgb_metrics['f1_weighted'] if xgb_metrics else 0,\n",
    "        cat_metrics['f1_weighted'] if cat_metrics else 0,\n",
    "        avg_val_metrics['f1_weighted'],\n",
    "        weighted_val_metrics['f1_weighted'],\n",
    "        vote_val_metrics['f1_weighted'],\n",
    "        stack_val_metrics['f1_weighted']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best method\n",
    "best_idx = comparison_df['F1 (Weighted)'].idxmax()\n",
    "best_method = comparison_df.loc[best_idx, 'Method']\n",
    "best_f1 = comparison_df.loc[best_idx, 'F1 (Weighted)']\n",
    "\n",
    "print(f\"\\nü•á Best Ensemble Method: {best_method}\")\n",
    "print(f\"   F1-Score (Weighted): {best_f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4fbf53",
   "metadata": {},
   "source": [
    "### FINAL TEST SET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use weighted average (usually best balance)\n",
    "print(f\"Using WEIGHTED AVERAGE ensemble for final evaluation\\n\")\n",
    "\n",
    "final_test_pred = ensemble_weighted_test_pred\n",
    "final_test_proba = ensemble_weighted_test_proba\n",
    "\n",
    "# Comprehensive evaluation\n",
    "test_accuracy = accuracy_score(y_test, final_test_pred)\n",
    "test_precision = precision_score(y_test, final_test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, final_test_pred, average='weighted')\n",
    "test_f1_macro = f1_score(y_test, final_test_pred, average='macro')\n",
    "test_f1_weighted = f1_score(y_test, final_test_pred, average='weighted')\n",
    "\n",
    "print(\"üèÜ FINAL ENSEMBLE PERFORMANCE:\\n\")\n",
    "print(f\"   Accuracy:           {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision (Avg):    {test_precision:.4f}\")\n",
    "print(f\"   Recall (Avg):       {test_recall:.4f}\")\n",
    "print(f\"   F1-Score (Macro):   {test_f1_macro:.4f}\")\n",
    "print(f\"   F1-Score (Weighted):{test_f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    final_test_pred,\n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, final_test_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax1.set_title('Final Ensemble - Test Confusion Matrix (Counts)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=ax2,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Proportion'})\n",
    "ax2.set_title('Final Ensemble - Test Confusion Matrix (Normalized)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/ensemble_model/confusion_matrix_final.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for i, (color, name) in enumerate(zip(colors, class_names)):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], final_test_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=color, lw=2,\n",
    "            label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Final Ensemble ROC Curves - Test Set', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/ensemble_model/roc_curves_final.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17852b",
   "metadata": {},
   "source": [
    "### SAVE ENSEMBLE COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model package\n",
    "ensemble_package = {\n",
    "    'models': {\n",
    "        'lightgbm': lgb_model,\n",
    "        'xgboost': xgb_model,\n",
    "        'catboost': cat_model\n",
    "    },\n",
    "    'weights': normalized_weights,\n",
    "    'meta_model': meta_model,\n",
    "    'method': 'weighted_average',  # Best performing method\n",
    "    'feature_engineering': engineer_transit_features  # Include function\n",
    "}\n",
    "\n",
    "# Save ensemble\n",
    "ensemble_path = f'{MODEL_PATH}/ensemble_model/ensemble_v1.pkl'\n",
    "with open(ensemble_path, 'wb') as f:\n",
    "    pickle.dump(ensemble_package, f)\n",
    "print(f\"‚úÖ Ensemble saved: {ensemble_path}\")\n",
    "\n",
    "# Save metadata\n",
    "ensemble_metadata = {\n",
    "    'version': '1.0',\n",
    "    'ensemble_method': 'weighted_average',\n",
    "    'base_models': list(models_loaded.keys()),\n",
    "    'weights': normalized_weights,\n",
    "    'performance': {\n",
    "        'validation': {\n",
    "            'accuracy': float(weighted_val_metrics['accuracy']),\n",
    "            'f1_macro': float(weighted_val_metrics['f1_macro']),\n",
    "            'f1_weighted': float(weighted_val_metrics['f1_weighted'])\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': float(test_accuracy),\n",
    "            'f1_macro': float(test_f1_macro),\n",
    "            'f1_weighted': float(test_f1_weighted),\n",
    "            'precision': float(test_precision),\n",
    "            'recall': float(test_recall)\n",
    "        }\n",
    "    },\n",
    "    'individual_models': {\n",
    "        'lightgbm': lgb_metrics if lgb_metrics else {},\n",
    "        'xgboost': xgb_metrics if xgb_metrics else {},\n",
    "        'catboost': cat_metrics if cat_metrics else {}\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = f'{MODEL_PATH}/ensemble_model/ensemble_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(ensemble_metadata, f, indent=4)\n",
    "print(f\"‚úÖ Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(f'{MODEL_PATH}/ensemble_model/ensemble_comparison.csv', index=False)\n",
    "print(f\"‚úÖ Comparison table saved\\n\")\n",
    "\n",
    "\n",
    "print(\"üìä Summary:\")\n",
    "print(f\"   Best Method:        Weighted Average\")\n",
    "print(f\"   Test Accuracy:      {test_accuracy*100:.2f}%\")\n",
    "print(f\"   F1-Score (Macro):   {test_f1_macro:.4f}\")\n",
    "print(f\"   Models Combined:    {sum(models_loaded.values())}/3\")\n",
    "print(f\"\\n   üöÄ Ready for deployment in web app!\")\n",
    "print(f\"   üìç Location: {ensemble_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
